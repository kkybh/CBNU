êµ°ì§‘í™”(Clustering)ëŠ” ë¹„ìŠ·í•œ íŠ¹ì„±ì„ ê°€ì§„ ë°ì´í„°ë“¤ì„ ê·¸ë£¹ìœ¼ë¡œ ë¬¶ëŠ” ë¹„ì§€ë„ í•™ìŠµ ê¸°ë²•ì´ì—ìš”. ì£¼ë¡œ ë°ì´í„° íƒìƒ‰, ì´ìƒ íƒì§€, ê³ ê° ì„¸ë¶„í™” ë“±ì— í™œìš©ë˜ì£ .

ğŸ” êµ°ì§‘í™” ê¸°ë³¸ ê°œë…
ëª©ì : ë ˆì´ë¸” ì—†ì´ ë°ì´í„° ë‚´ì¬ êµ¬ì¡°ë¥¼ ë°œê²¬

ì…ë ¥: íŠ¹ì§• ë²¡í„°(íŠ¹ì§• ê³µê°„)

ì¶œë ¥: êµ°ì§‘(cluster) ë ˆì´ë¸”

ì£¼ìš” êµ°ì§‘í™” ì•Œê³ ë¦¬ì¦˜
ì•Œê³ ë¦¬ì¦˜	íŠ¹ì§•	ì‚¬ìš© ì‚¬ë¡€
K-í‰ê·  (K-Means)	ì¤‘ì‹¬ ê¸°ë°˜, ê°„ë‹¨í•˜ê³  ë¹ ë¦„, êµ°ì§‘ ê°œìˆ˜ í•„ìš”	ê³ ê° ì„¸ë¶„í™”, ë¬¸ì„œ ë¶„ë¥˜ ë“±
ê³„ì¸µì  êµ°ì§‘í™” (Hierarchical)	íŠ¸ë¦¬ í˜•íƒœ êµ°ì§‘ ìƒì„±, êµ°ì§‘ ìˆ˜ ë¯¸ì • ê°€ëŠ¥	ìœ ì „ì ë¶„ë¥˜, ë¬¸ì„œ ì§‘í•© ë¶„ì„
DBSCAN	ë°€ë„ ê¸°ë°˜, ë…¸ì´ì¦ˆ(ì´ìƒì¹˜) ì²˜ë¦¬ ê°€ëŠ¥	ì´ìƒ íƒì§€, ì§€ë¦¬ ë°ì´í„° ë¶„ì„
Mean Shift	ë°€ë„ ë´‰ìš°ë¦¬ íƒìƒ‰, êµ°ì§‘ ìˆ˜ ìë™ ê²°ì •	ì´ë¯¸ì§€ ì²˜ë¦¬, ê°ì²´ ì¶”ì 

ê°„ë‹¨í•œ K-Means ì˜ˆì œ (Python)
python
ë³µì‚¬
í¸ì§‘
from sklearn.cluster import KMeans
import numpy as np

# ì˜ˆì‹œ ë°ì´í„°: 2ì°¨ì› ì¢Œí‘œ 10ê°œ
X = np.array([[1, 2], [1, 4], [1, 0],
              [4, 2], [4, 4], [4, 0],
              [10, 2], [10, 4], [10, 0]])

# êµ°ì§‘ ê°œìˆ˜ 3ê°œë¡œ KMeans ì ìš©
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(X)

print("í´ëŸ¬ìŠ¤í„° ë ˆì´ë¸”:", kmeans.labels_)
print("í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì :", kmeans.cluster_centers_)
êµ°ì§‘í™” í™œìš© ì˜ˆ
ê³ ê° ì„¸ë¶„í™”: ë§ˆì¼€íŒ… íƒ€ê²Ÿ ê·¸ë£¹ ë¶„ë¥˜

ì´ìƒì¹˜ íƒì§€: ë°€ë„ ê¸°ë°˜ êµ°ì§‘ ì™¸ë¶€ ë°ì´í„° ì´ìƒ ê°ì§€

ì´ë¯¸ì§€ ì²˜ë¦¬: ìœ ì‚¬ ìƒ‰ìƒ/íŒ¨í„´ ê·¸ë£¹í™”

ë¬¸ì„œ êµ°ì§‘í™”: ì£¼ì œë³„ ë¬¸ì„œ ê·¸ë£¹ ìƒì„±

ì˜ˆì‹œëŠ” ìœ ëª…í•œ Iris ê½ƒ ë°ì´í„°ì…‹ì„ ì´ìš©í•´ K-Means êµ°ì§‘í™”ë¥¼ ìˆ˜í–‰í•˜ëŠ” ì½”ë“œ

1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸° ë° ë°ì´í„° ì¤€ë¹„
python
ë³µì‚¬
í¸ì§‘
from sklearn.datasets import load_iris
from sklearn.cluster import KMeans
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Iris ë°ì´í„°ì…‹ ë¡œë“œ
iris = load_iris()
X = iris.data  # ê½ƒë°›ì¹¨, ê½ƒì ê¸¸ì´/ë„ˆë¹„ ë“± 4ê°œ íŠ¹ì§•
df = pd.DataFrame(X, columns=iris.feature_names)
2. K-Means êµ°ì§‘í™” ìˆ˜í–‰
python
ë³µì‚¬
í¸ì§‘
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(X)
df['cluster'] = kmeans.labels_
3. êµ°ì§‘ ê²°ê³¼ ì‹œê°í™” (ì£¼ì„±ë¶„ë¶„ì„ìœ¼ë¡œ 2ì°¨ì› ì¶•ì†Œ)
python
ë³µì‚¬
í¸ì§‘
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
components = pca.fit_transform(X)
df['pca1'] = components[:, 0]
df['pca2'] = components[:, 1]

plt.figure(figsize=(8,6))
sns.scatterplot(data=df, x='pca1', y='pca2', hue='cluster', palette='Set1', s=100)
plt.title('Iris Dataset Clustering (KMeans, k=3)')
plt.show()
4. êµ°ì§‘ í’ˆì§ˆ í‰ê°€ (ì‹¤ì œ ë¼ë²¨ê³¼ ë¹„êµ)
python
ë³µì‚¬
í¸ì§‘
from sklearn.metrics import adjusted_rand_score

labels_true = iris.target
labels_pred = kmeans.labels_
ari = adjusted_rand_score(labels_true, labels_pred)
print(f'Adjusted Rand Index: {ari:.3f}')  # 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ êµ°ì§‘ê³¼ ì‹¤ì œ ë¼ë²¨ ìœ ì‚¬
ìš”ì•½
Iris ë°ì´í„°ì˜ 4ì°¨ì› íŠ¹ì§•ì„ 2ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œí•´ ì‹œê°í™”

KMeansê°€ 3ê°œì˜ êµ°ì§‘ìœ¼ë¡œ ë¶„ë¥˜í•¨

ARI ì§€í‘œë¡œ êµ°ì§‘ ì •í™•ë„ í‰ê°€

ì‹¤ì œ ê³ ê° ë°ì´í„°ì™€ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ê°ê° êµ°ì§‘í™”í•˜ëŠ” ë°©ë²•ì„ ê°„ë‹¨íˆ ì„¤ëª…
1. ê³ ê° ë°ì´í„° êµ°ì§‘í™”
ë°ì´í„° íŠ¹ì§•
ê³ ê°ë³„ êµ¬ë§¤ ì´ë ¥, ë‚˜ì´, ì§€ì—­, ì†Œë¹„ íŒ¨í„´ ë“± ì—¬ëŸ¬ í”¼ì²˜ ì¡´ì¬

ëŒ€ê°œ ìˆ˜ì¹˜í˜•+ë²”ì£¼í˜• ë°ì´í„° í˜¼í•©

ì „ì²˜ë¦¬ ë° íŠ¹ì§•
ë²”ì£¼í˜• ë³€ìˆ˜ â†’ ì›-í•« ì¸ì½”ë”©(One-Hot Encoding)

ìˆ˜ì¹˜í˜• ë³€ìˆ˜ â†’ ì •ê·œí™” (MinMaxScaler, StandardScaler)

ê²°ì¸¡ì¹˜ ì²˜ë¦¬, ì´ìƒì¹˜ ì œê±° í•„ìˆ˜

êµ°ì§‘í™” ì•Œê³ ë¦¬ì¦˜ ì¶”ì²œ
K-Means: ë¹ ë¥´ê³  ì§ê´€ì , ìˆ˜ì¹˜í˜• ë°ì´í„°ì— ì í•©

GMM (Gaussian Mixture Model): êµ°ì§‘ ê²½ê³„ê°€ ê²¹ì¹  ë•Œ í™œìš©

DBSCAN: ì´ìƒì¹˜(ë…¸ì´ì¦ˆ) ë‹¤ìˆ˜ì¸ ê²½ìš° ê°•ì 

ì˜ˆì‹œ ì½”ë“œ (K-Means)
python
ë³µì‚¬
í¸ì§‘
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

# ê³ ê° ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
df = pd.read_csv('customer_data.csv')

# ì˜ˆ: 'gender' ë²”ì£¼í˜• -> ì›-í•« ì¸ì½”ë”©
df = pd.get_dummies(df, columns=['gender'])

# ìˆ˜ì¹˜í˜• í”¼ì²˜ ì„ íƒ ë° ì •ê·œí™”
features = ['age', 'annual_income', 'spending_score']
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df[features])

# KMeans êµ°ì§‘í™”
kmeans = KMeans(n_clusters=4, random_state=42)
kmeans.fit(X_scaled)

df['cluster'] = kmeans.labels_
2. ì´ë¯¸ì§€ ë°ì´í„° êµ°ì§‘í™”
ë°ì´í„° íŠ¹ì§•
ì´ë¯¸ì§€ ìì²´ëŠ” í”½ì…€ê°’ ë‹¤ì°¨ì› ë°°ì—´(ê³ ì°¨ì›)

ì§ì ‘ êµ°ì§‘í™” ì‹œ ê³„ì‚°ëŸ‰ ë§ê³  ì„±ëŠ¥ ì €í•˜

ì ‘ê·¼ë²•
íŠ¹ì§• ì¶”ì¶œ: CNN ê°™ì€ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ í™œìš©í•´ ì´ë¯¸ì§€ íŠ¹ì§• ë²¡í„°(ì„ë² ë”©) ì¶”ì¶œ

ì°¨ì› ì¶•ì†Œ: PCA, t-SNE ë“±ìœ¼ë¡œ ì°¨ì› ì¶•ì†Œ ê°€ëŠ¥

êµ°ì§‘í™”: K-Means, DBSCAN ë“± ì ìš©

ì˜ˆì‹œ: ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ í›„ K-Means
python
ë³µì‚¬
í¸ì§‘
import numpy as np
from tensorflow.keras.applications import VGG16
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.applications.vgg16 import preprocess_input
from sklearn.cluster import KMeans
import os

# ì‚¬ì „í•™ìŠµëœ VGG16 ëª¨ë¸ (FCì¸µ ì œì™¸, íŠ¹ì§• ì¶”ì¶œìš©)
model = VGG16(weights='imagenet', include_top=False, pooling='avg')

def extract_features(image_path):
    img = load_img(image_path, target_size=(224, 224))
    x = img_to_array(img)
    x = np.expand_dims(x, axis=0)
    x = preprocess_input(x)
    features = model.predict(x)
    return features.flatten()

# ì´ë¯¸ì§€ í´ë” ë‚´ ëª¨ë“  ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ
image_dir = 'images/'
features_list = []
image_names = []

for fname in os.listdir(image_dir):
    if fname.endswith('.jpg'):
        fpath = os.path.join(image_dir, fname)
        features = extract_features(fpath)
        features_list.append(features)
        image_names.append(fname)

features_array = np.array(features_list)

# KMeans êµ°ì§‘í™”
kmeans = KMeans(n_clusters=5, random_state=42)
kmeans.fit(features_array)
labels = kmeans.labels_

# ê²°ê³¼ ì¶œë ¥
for name, label in zip(image_names, labels):
    print(f'{name}: cluster {label}')

    
