GIS(ì§€ë¦¬ì •ë³´ì‹œìŠ¤í…œ)ì—ì„œ ê³µê°„ ì˜ì—­(Area) ë°ì´í„° ì²˜ë¦¬ì— ëŒ€í•´ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…ë“œë¦´ê²Œìš”. ê³µê°„ ì˜ì—­ ì²˜ë¦¬ëŠ” ì§€ë¦¬ ë°ì´í„°ì—ì„œ íŠ¹ì • ì§€ì—­ì˜ ë©´ì  ê³„ì‚°, ê³µê°„ ì—°ì‚°, ì†ì„± ë¶„ì„ ë“±ì„ í¬í•¨í•©ë‹ˆë‹¤.

1. ì£¼ìš” ê°œë… ë° ë¼ì´ë¸ŒëŸ¬ë¦¬
ê³µê°„ ì˜ì—­ (Area) : í´ë¦¬ê³¤(ë‹¤ê°í˜•) í˜•íƒœë¡œ í‘œí˜„ë˜ë©°, ë©´ì  ê³„ì‚°ì´ í•µì‹¬

ì¢Œí‘œê³„ (Coordinate Reference System, CRS) : ì •í™•í•œ ë©´ì  ê³„ì‚° ìœ„í•´ ì ì ˆí•œ íˆ¬ì˜ë²• ì‚¬ìš© í•„ìš”

ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬:

Python: geopandas, shapely, pyproj

GIS íˆ´: QGIS, ArcGIS ë“±

2. Python ì˜ˆì œ: GeoPandasë¡œ ë©´ì  ê³„ì‚°
python
ë³µì‚¬
í¸ì§‘
import geopandas as gpd

# ì˜ˆì‹œ: ìƒ˜í”Œ í´ë¦¬ê³¤ ë°ì´í„° ìƒì„±
from shapely.geometry import Polygon

poly = Polygon([(0, 0), (1, 0), (1, 1), (0, 1)])  # 1x1 ì •ì‚¬ê°í˜•
gdf = gpd.GeoDataFrame({'geometry': [poly]}, crs="EPSG:4326")  # WGS84 ì¢Œí‘œê³„

# ë©´ì  ê³„ì‚° ì „, ì ì ˆí•œ íˆ¬ì˜ë²•ìœ¼ë¡œ ë³€í™˜ í•„ìš” (ë‹¨ìœ„: m)
gdf_proj = gdf.to_crs(epsg=3857)  # Web Mercator íˆ¬ì˜ë²• (ë¯¸í„° ë‹¨ìœ„)

# ë©´ì  ê³„ì‚° (m^2 ë‹¨ìœ„)
gdf_proj['area_m2'] = gdf_proj['geometry'].area
print(gdf_proj[['geometry', 'area_m2']])
3. ê³µê°„ ì˜ì—­ ì²˜ë¦¬ ì£¼ìš” ì‘ì—…
ì‘ì—…	ì„¤ëª…	ì˜ˆì‹œ
ë©´ì  ê³„ì‚°	í´ë¦¬ê³¤ì˜ ì‹¤ì œ ë©´ì  ì‚°ì¶œ	geometry.area (íˆ¬ì˜ ì¢Œí‘œê³„ì—ì„œ)
ë²„í¼(Buffer)	ì˜ì—­ ì£¼ë³€ ì¼ì • ê±°ë¦¬ë§Œí¼ í™•ì¥	geometry.buffer(distance)
êµì§‘í•©(Intersection)	ë‘ ì˜ì—­ì´ ê²¹ì¹˜ëŠ” ë¶€ë¶„ ê³„ì‚°	geom1.intersection(geom2)
í•©ì§‘í•©(Union)	ë‘ ì˜ì—­ í•©ì¹¨	geom1.union(geom2)
ì°¨ì§‘í•©(Difference)	ì˜ì—­ì—ì„œ ë‹¤ë¥¸ ì˜ì—­ ì œì™¸	geom1.difference(geom2)

4. ì‹¤ì œ ë°ì´í„° ì ìš© íŒ
ì¢Œí‘œê³„ ì£¼ì˜: ê²½ë„/ìœ„ë„ ì¢Œí‘œê³„(WGS84)ëŠ” ë©´ì  ê³„ì‚° ë¶€ì í•© â†’ íˆ¬ì˜ ì¢Œí‘œê³„ë¡œ ë³€í™˜ í•„ìˆ˜

ëŒ€ê·œëª¨ ë°ì´í„° ì²˜ë¦¬: ê³µê°„ ì¸ë±ì‹±(R-tree), ë³‘ë ¬ì²˜ë¦¬ í™œìš©

ì‹œê°í™”: geopandas.plot() ë˜ëŠ” Folium, Plotly ë“±ìœ¼ë¡œ ì§€ë„ ì‹œê°í™” ê°€ëŠ¥

 ê³µê°„ ì˜ì—­ ë°ì´í„° ì „ì²˜ë¦¬

 1. ê³µê°„ ì˜ì—­ ë°ì´í„° ì „ì²˜ë¦¬ ì£¼ìš” ì‘ì—…
ì‘ì—…	ì„¤ëª…	ì˜ˆì‹œ/ë°©ë²•
1. ì¢Œí‘œê³„ í†µì¼ (CRS í†µì¼)	ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì˜ ì¢Œí‘œê³„ í†µì¼	to_crs()ë¡œ í†µì¼ (ì˜ˆ: EPSG:3857)
2. ê²°ì¸¡ì¹˜/ì˜ëª»ëœ ì§€ì˜¤ë©”íŠ¸ë¦¬ ì²˜ë¦¬	ëˆ„ë½ ë°ì´í„° ì œê±°, ì˜ëª»ëœ í´ë¦¬ê³¤ ìˆ˜ì •	dropna(), buffer(0)ë¡œ ì˜¤ë¥˜ ìˆ˜ì •
3. ì¤‘ë³µ ë° ê²¹ì¹¨ ì œê±°	ì¤‘ë³µ í”¼ì²˜ ì œê±°, ì¤‘ì²© ì˜ì—­ ì •ë¦¬	drop_duplicates(), overlay() í™œìš©
4. ë‹¨ìˆœí™” (Simplification)	ë³µì¡í•œ í´ë¦¬ê³¤ ì  ìˆ˜ ì¤„ì—¬ ì—°ì‚° ì†ë„ í–¥ìƒ	simplify(tolerance) í•¨ìˆ˜ ì‚¬ìš©
5. ë²„í¼ë§ ë° í´ë¦¬í•‘	íŠ¹ì • ì˜ì—­ í™•ì¥ ë˜ëŠ” ìë¥´ê¸°	buffer(distance), clip() ì‚¬ìš©
6. ì†ì„± ë°ì´í„° ì •ë¦¬	í•„ìš”ì—†ëŠ” ì»¬ëŸ¼ ì œê±°, ë°ì´í„° íƒ€ì… ë³€í™˜	drop(), astype() ë“±

2. Python ì˜ˆì œ: GeoPandasë¡œ ê³µê°„ ì˜ì—­ ë°ì´í„° ì „ì²˜ë¦¬
python
ë³µì‚¬
í¸ì§‘
import geopandas as gpd

# 1) ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
gdf = gpd.read_file('your_spatial_data.shp')

# 2) ì¢Œí‘œê³„ í™•ì¸ ë° í†µì¼ (ì˜ˆ: EPSG:4326 -> EPSG:3857)
print("ì›ë³¸ CRS:", gdf.crs)
gdf = gdf.to_crs(epsg=3857)

# 3) ê²°ì¸¡ì¹˜ ë° ì˜ëª»ëœ ì§€ì˜¤ë©”íŠ¸ë¦¬ ì œê±°/ìˆ˜ì •
gdf = gdf.dropna(subset=['geometry'])
gdf['geometry'] = gdf['geometry'].buffer(0)  # ìê°€ êµì°¨ ë“± ì˜¤ë¥˜ ìˆ˜ì •

# 4) ì¤‘ë³µ í”¼ì²˜ ì œê±°
gdf = gdf.drop_duplicates()

# 5) ë‹¨ìˆœí™” ì²˜ë¦¬ (tolerance: í—ˆìš© ì˜¤ì°¨)
gdf['geometry'] = gdf['geometry'].simplify(tolerance=10)

# 6) í•„ìš”ì—†ëŠ” ì»¬ëŸ¼ ì œê±°
gdf = gdf.drop(columns=['unnecessary_column'])

# 7) ë²„í¼ë§ ì˜ˆì‹œ (ì˜ì—­ 100m í™•ì¥)
gdf['geometry_buffered'] = gdf.geometry.buffer(100)

# ê²°ê³¼ ì €ì¥
gdf.to_file('processed_data.shp')
3. ì¶”ê°€ íŒ
ìê°€êµì°¨ ì˜¤ë¥˜: buffer(0)ì´ ê°„ë‹¨í•œ ì˜¤ë¥˜ ìˆ˜ì •ì— íš¨ê³¼ì 

ë³µì¡ë„ ì¡°ì ˆ: simplify()ì˜ preserve_topology=True ì˜µì…˜ìœ¼ë¡œ ìœ„ìƒ ìœ ì§€ ê°€ëŠ¥

ê³µê°„ ì¡°ì¸: sjoin()ìœ¼ë¡œ ê³µê°„ ê¸°ë°˜ ë°ì´í„° ê²°í•© ê°€ëŠ¥

ëŒ€ìš©ëŸ‰ ì²˜ë¦¬: dask-geopandas ë“± ë³‘ë ¬ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™œìš© ê³ ë ¤

ëŒ€ìš©ëŸ‰ ê³µê°„ ë°ì´í„° ì²˜ë¦¬ë²•

1. ëŒ€ìš©ëŸ‰ ê³µê°„ ë°ì´í„° ì²˜ë¦¬ ì „ëµ
ì „ëµ	ì„¤ëª…
ê³µê°„ ì¸ë±ìŠ¤ ì‚¬ìš©	R-tree, QuadTree ë“± ê³µê°„ ê²€ìƒ‰ì„ ë¹ ë¥´ê²Œ í•˜ê¸° ìœ„í•œ ìë£Œêµ¬ì¡°
ì¢Œí‘œê³„ ìµœì í™”	íˆ¬ì˜ ì¢Œí‘œê³„ë¡œ ë³€í™˜ í›„ ê±°ë¦¬/ë©´ì  ì—°ì‚° (ê³„ì‚° ë¶€í•˜ â†“)
ë¶„í•  ì²˜ë¦¬ (Chunking)	ëŒ€ìš©ëŸ‰ GeoDataFrameì„ ë¶€ë¶„ì ìœ¼ë¡œ ì²˜ë¦¬
ë³‘ë ¬ ì²˜ë¦¬	ì—¬ëŸ¬ CPU ì½”ì–´ë¡œ ë¶„ì‚° ì²˜ë¦¬
íŒŒì¼ í¬ë§· ìµœì í™”	Feather, Parquet, GeoPackage ë“± ë¹ ë¥¸ I/O í˜•ì‹ ì‚¬ìš©
Dask or PostGIS ì‚¬ìš©	ëŒ€ìš©ëŸ‰ ë°ì´í„° ë¶„ì‚° ì²˜ë¦¬ ë„êµ¬ í™œìš©

âœ… 2. ì£¼ìš” ë„êµ¬/ë¼ì´ë¸ŒëŸ¬ë¦¬
ë„êµ¬	ì„¤ëª…
GeoPandas + Rtree	ì†Œê·œëª¨~ì¤‘ê°„ ê·œëª¨ì— ì í•©, ê³µê°„ ì¸ë±ì‹± ì§€ì›
Dask-GeoPandas	GeoPandasì˜ ë³‘ë ¬ í™•ì¥ ë²„ì „
Pyogrio, Fiona	ë¹ ë¥¸ GeoFile I/O ì²˜ë¦¬
Spatialite, PostGIS	ê³µê°„ ë°ì´í„°ë² ì´ìŠ¤ ì²˜ë¦¬
GeoSpark / Apache Sedona	Spark ê¸°ë°˜ ê³µê°„ ë¹…ë°ì´í„° ì²˜ë¦¬ í”„ë ˆì„ì›Œí¬
TileDB, H3	ë²¡í„°/ê²©ì ê¸°ë°˜ ê³µê°„ ì¸ë±ì‹± ì²˜ë¦¬ ë„êµ¬

âœ… 3. ì‹¤ìš© ì˜ˆì œ
ğŸ§© ì˜ˆì‹œ 1: ê³µê°„ ì¸ë±ì‹± + í´ë¦¬í•‘ ì²˜ë¦¬ (GeoPandas + Rtree)
python
ë³µì‚¬
í¸ì§‘
import geopandas as gpd
from shapely.geometry import box

# ëŒ€ìš©ëŸ‰ GeoDataFrame ë¶ˆëŸ¬ì˜¤ê¸°
gdf = gpd.read_file('large_dataset.shp')

# ê³µê°„ ì¸ë±ì‹± ìë™ ì ìš©
clip_box = box(126.8, 37.4, 127.2, 37.7)  # ì„œìš¸ íŠ¹ì • ì˜ì—­
gdf_clipped = gdf[gdf.intersects(clip_box)]

# í•„í„°ë§ í›„ ë©´ì  ê³„ì‚°
gdf_clipped = gdf_clipped.to_crs(epsg=3857)
gdf_clipped['area'] = gdf_clipped.area
ğŸ§© ì˜ˆì‹œ 2: Dask-GeoPandasë¡œ ë³‘ë ¬ ì²˜ë¦¬
python
ë³µì‚¬
í¸ì§‘
import dask_geopandas as dgpd

dask_gdf = dgpd.read_parquet('large_spatial_data.parquet')

# ì˜ˆ: ì§€ì—­ ë‚´ êµì°¨ ì˜ì—­ë§Œ í•„í„°ë§ (R-tree ë³‘ë ¬ ì ìš©ë¨)
from shapely.geometry import box
roi = box(129.1, 35.1, 129.3, 35.3)
roi_gdf = gpd.GeoDataFrame(geometry=[roi], crs="EPSG:4326")

# ê³µê°„ í•„í„°
result = dask_gdf.sjoin(roi_gdf, how="inner", predicate="intersects")
result.compute().to_file('result.shp')
ğŸ§© ì˜ˆì‹œ 3: PostGISì—ì„œ SQLë¡œ ê³µê°„ ì¿¼ë¦¬
sql
ë³µì‚¬
í¸ì§‘
-- íŠ¹ì • ì§€ì—­ ë‚´ ë„ë¡œ í•„í„°ë§
SELECT *
FROM roads
WHERE ST_Intersects(roads.geom, ST_MakeEnvelope(126.8, 37.4, 127.2, 37.7, 4326));
âœ… 4. ì„±ëŠ¥ ìµœì í™” íŒ
íŒ	ì„¤ëª…
GeoDataFrame .to_crs()	íˆ¬ì˜ ì¢Œí‘œê³„ ë³€í™˜ í›„ ì—°ì‚° (ë©´ì , ê±°ë¦¬ ì •í™•ë„ â†‘)
.simplify(tolerance)	ë¶ˆí•„ìš”í•œ ì •ì  ì œê±°í•´ ì„±ëŠ¥ ê°œì„ 
to_parquet()	ëŒ€ìš©ëŸ‰ íŒŒì¼ ì €ì¥/ë¶ˆëŸ¬ì˜¤ê¸° ì†ë„ â†‘
spatial indexing (R-tree)	ê³µê°„ ì¿¼ë¦¬ ì†ë„ ìˆ˜ì‹­ ë°° ê°œì„  ê°€ëŠ¥
PostGIS	ìˆ˜ì–µ ê°œ ê°ì²´ ì²˜ë¦¬ë„ ì•ˆì •ì  (ì¸ë±ìŠ¤, íŠœë‹ í•„ìš”)

ğŸ” ì •ë¦¬
ì†Œê·œëª¨~ì¤‘ê°„ ê·œëª¨ â†’ GeoPandas + Rtree

ìˆ˜ì‹­ë§Œ~ìˆ˜ë°±ë§Œ ê°ì²´ â†’ Dask-GeoPandas, PostGIS

ìˆ˜ì–µ ê±´ ì´ìƒ â†’ Apache Sedona, Spark + HDFS + GeoParquet

1. Apache Sedona ì†Œê°œ
íŠ¹ì§•	ì„¤ëª…
í”Œë«í¼	Apache Spark ê¸°ë°˜ì˜ ë¶„ì‚° ê³µê°„ ë¶„ì„ ì—”ì§„
ì§€ì› í¬ë§·	Shapefile, GeoJSON, CSV, WKT, WKB ë“±
ì œê³µ ê¸°ëŠ¥	ê³µê°„ ì¡°ì¸, ë²„í¼ë§, êµì°¨/í¬í•¨ ë¶„ì„, í´ëŸ¬ìŠ¤í„°ë§ ë“±
ì§€ì› ì–¸ì–´	Scala, Java, Python (PySpark)
ê³µê°„ ì¸ë±ìŠ¤	R-Tree, QuadTree ì§€ì›
CRS ì§€ì›	ì¢Œí‘œê³„ ë³€í™˜, GeoTools í†µí•©

âœ… 2. ì„¤ì¹˜ ë°©ë²• (PySpark ê¸°ì¤€)
ğŸ”¹ PySpark í™˜ê²½ì—ì„œ ì„¤ì¹˜
bash
ë³µì‚¬
í¸ì§‘
pip install apache-sedona
ë˜ëŠ” Spark ì‹¤í–‰ ì‹œ Sedona íŒ¨í‚¤ì§€ ë¡œë“œ:

bash
ë³µì‚¬
í¸ì§‘
pyspark \
  --packages org.apache.sedona:sedona-python-adapter-3.0_2.12:1.4.1-incubating \
  --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
  --conf spark.kryo.registrator=org.apache.sedona.core.serde.SedonaKryoRegistrator
ë²„ì „ì€ Spark ë° Pythonì— ë§ê²Œ ì¡°ì • í•„ìš”

âœ… 3. PySpark + Sedona ê¸°ë³¸ ì˜ˆì œ
ğŸ”¹ ì´ˆê¸°í™” ë° ë“±ë¡
python
ë³µì‚¬
í¸ì§‘
from sedona.register import SedonaRegistrator
from sedona.utils import SedonaKryoRegistrator, KryoSerializer

from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("SedonaExample") \
    .config("spark.serializer", KryoSerializer.getName) \
    .config("spark.kryo.registrator", SedonaKryoRegistrator.getName) \
    .getOrCreate()

SedonaRegistrator.registerAll(spark)
ğŸ”¹ 1. ê³µê°„ ë°ì´í„° ë¡œë“œ ë° íŒŒì‹±
python
ë³µì‚¬
í¸ì§‘
from sedona.sql.types import GeometryType
from pyspark.sql.functions import col, expr

# ì˜ˆ: WKT í˜•ì‹ CSV ë¶ˆëŸ¬ì˜¤ê¸°
df = spark.read.option("header", "true").csv("points.csv")

# WKT â†’ Geometry ë³€í™˜
df = df.withColumn("geometry", expr("ST_GeomFromWKT(wkt_column)"))
df.createOrReplaceTempView("points_view")
ğŸ”¹ 2. ê³µê°„ ì—°ì‚° (ì˜ˆ: í¬í•¨ëœ í¬ì¸íŠ¸ ì°¾ê¸°)
python
ë³µì‚¬
í¸ì§‘
# ì‚¬ê°í˜• ROI ì •ì˜ (WKT)
roi = "POLYGON((127.0 37.0, 127.1 37.0, 127.1 37.1, 127.0 37.1, 127.0 37.0))"

# ROI â†’ Geometry
spark.sql(f"SELECT ST_GeomFromText('{roi}') AS roi_geom").createOrReplaceTempView("roi")

# ê³µê°„ í¬í•¨ ì¿¼ë¦¬
result = spark.sql("""
SELECT p.*
FROM points_view p, roi
WHERE ST_Contains(roi.roi_geom, p.geometry)
""")
result.show()
ğŸ”¹ 3. ê³µê°„ ì¸ë±ìŠ¤ ë° íŒŒí‹°ì…”ë‹ ì ìš© (ê³ ì† ì²˜ë¦¬)
python
ë³µì‚¬
í¸ì§‘
from sedona.core.SpatialRDD import PointRDD
from sedona.core.enums import IndexType, GridType

rdd = PointRDD(
    rawSpatialRDD=result.rdd,
    storageLevel="MEMORY_ONLY"
)

# íŒŒí‹°ì…”ë‹ + ì¸ë±ìŠ¤ ì„¤ì •
rdd.analyze()
rdd.spatialPartitioning(GridType.KDBTREE)
rdd.buildIndex(IndexType.RTREE, True)
âœ… 4. Sedona ì§€ì› ì—°ì‚° ì˜ˆì‹œ
ì—°ì‚°	SQL í•¨ìˆ˜
êµì°¨ ì—¬ë¶€	ST_Intersects(geom1, geom2)
í¬í•¨ ì—¬ë¶€	ST_Contains(geom1, geom2)
ê±°ë¦¬ ê³„ì‚°	ST_Distance(geom1, geom2)
ë²„í¼ ìƒì„±	ST_Buffer(geom, dist)
ë©´ì  ê³„ì‚°	ST_Area(geom)
ê²¹ì¹¨ ë¶€ë¶„ ì¶”ì¶œ	ST_Intersection(geom1, geom2)

âœ… 5. í™•ì¥ ì ìš© ì˜ˆì‹œ
ìˆ˜ì–µ ê°œ GPS í¬ì¸íŠ¸ í•„í„°ë§

ë„ë¡œë§ê³¼ ì¸êµ¬ ê²©ì ë§¤ì¹­

ìœ„í—˜ ì§€ì—­ í´ëŸ¬ìŠ¤í„°ë§ (DBSCAN)

ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° + ê³µê°„ í•„í„°ë§ (Kafka + Sedona + Spark Structured Streaming)

âœ… ì°¸ê³  ìë£Œ
Sedona ê³µì‹ ë¬¸ì„œ (Python)

Sedona SQL í•¨ìˆ˜ ëª©ë¡

ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ë°ì´í„°ì™€ ê²°í•©
ì „ì²´ êµ¬ì„± ì‹œë‚˜ë¦¬ì˜¤
mermaid
ë³µì‚¬
í¸ì§‘
graph TD
    A[Kafka ë˜ëŠ” ì†Œì¼“ ì…ë ¥] --> B[Structured Streaming]
    B --> C[Sedona: ê³µê°„ íŒŒì‹± ë° ì—°ì‚°]
    C --> D[ê³µê°„ ì¡°ê±´ í•„í„°ë§ (ST_Contains, ST_Intersects ë“±)]
    D --> E[ê²°ê³¼ ì €ì¥ ë˜ëŠ” ì•Œë¦¼ (Console, DB, File, API)]
âœ… ì˜ˆì œ ì‹œë‚˜ë¦¬ì˜¤: Kafkaë¡œ ë“¤ì–´ì˜¤ëŠ” GPS ë°ì´í„°ë¥¼ íŠ¹ì • êµ¬ì—­(Polygon)ê³¼ ë¹„êµí•˜ì—¬ í•„í„°ë§
ğŸ”¹ 1. í™˜ê²½ ì„¤ì •
bash
ë³µì‚¬
í¸ì§‘
pip install apache-sedona
pip install kafka-python
bash
ë³µì‚¬
í¸ì§‘
# Spark ì‹¤í–‰ ì‹œ íŒ¨í‚¤ì§€ í¬í•¨
pyspark --packages org.apache.sedona:sedona-python-adapter-3.0_2.12:1.4.1-incubating \
        --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
        --conf spark.kryo.registrator=org.apache.sedona.core.serde.SedonaKryoRegistrator
ğŸ”¹ 2. ì½”ë“œ ì˜ˆì‹œ (PySpark + Sedona + Kafka)
python
ë³µì‚¬
í¸ì§‘
from pyspark.sql import SparkSession
from pyspark.sql.functions import expr
from sedona.register import SedonaRegistrator
from sedona.utils import SedonaKryoRegistrator, KryoSerializer

spark = SparkSession.builder \
    .appName("RealTimeGeoProcessing") \
    .config("spark.serializer", KryoSerializer.getName) \
    .config("spark.kryo.registrator", SedonaKryoRegistrator.getName) \
    .getOrCreate()

SedonaRegistrator.registerAll(spark)
ğŸ”¹ 3. Kafkaë¡œë¶€í„° ìŠ¤íŠ¸ë¦¼ ë°ì´í„° ìˆ˜ì‹ 
python
ë³µì‚¬
í¸ì§‘
# Kafka ìŠ¤íŠ¸ë¦¬ë° ì…ë ¥
stream_df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "gps_topic") \
    .load()

# JSON íŒŒì‹± ì˜ˆ: {"id": "device1", "lat": 37.5, "lon": 127.03}
from pyspark.sql.functions import from_json, col
from pyspark.sql.types import StructType, DoubleType, StringType

schema = StructType() \
    .add("id", StringType()) \
    .add("lat", DoubleType()) \
    .add("lon", DoubleType())

gps_data = stream_df.selectExpr("CAST(value AS STRING)") \
    .select(from_json(col("value"), schema).alias("data")) \
    .select("data.*")
ğŸ”¹ 4. ì‹¤ì‹œê°„ ê³µê°„ í•„í„°ë§
python
ë³µì‚¬
í¸ì§‘
# WKTë¡œ Polygon ì˜ì—­ ì •ì˜
roi_wkt = "POLYGON((127.0 37.0, 127.2 37.0, 127.2 37.2, 127.0 37.2, 127.0 37.0))"
roi_df = spark.sql(f"SELECT ST_GeomFromText('{roi_wkt}') AS roi_geom")
roi_df.createOrReplaceTempView("roi")

# GPS ì¢Œí‘œë¥¼ Point Geometryë¡œ ë³€í™˜
gps_data = gps_data.withColumn("geom", expr("ST_Point(cast(lon as Decimal(24,20)), cast(lat as Decimal(24,20)))"))
gps_data.createOrReplaceTempView("gps_view")

# ê³µê°„ í¬í•¨ í•„í„°ë§
result = spark.sql("""
SELECT g.*
FROM gps_view g, roi
WHERE ST_Contains(roi.roi_geom, g.geom)
""")
ğŸ”¹ 5. ì‹¤ì‹œê°„ ì¶œë ¥ (Console or Sink)
python
ë³µì‚¬
í¸ì§‘
query = result.writeStream \
    .outputMode("append") \
    .format("console") \
    .start()

query.awaitTermination()
âœ… ì‹¤ì‹œê°„ ê²°í•© í™•ì¥ ì‘ìš©
ì‘ìš© ì‚¬ë¡€	ì„¤ëª…
ì‹¤ì‹œê°„ ì°¨ëŸ‰ ìœ„ì¹˜ ì¶”ì 	ì§€ì •ëœ êµ¬ì—­ ì§„ì… ì•Œë¦¼
ì„¼ì„œ ê¸°ë°˜ ì´ìƒ íƒì§€	ìœ„í—˜ êµ¬ì—­ ë‚´ ì´ìƒ ìƒíƒœ ê°ì§€
ì‹¤ì‹œê°„ ê³µê°„ í´ëŸ¬ìŠ¤í„°ë§	DBSCAN ë“± ì ìš© ê°€ëŠ¥ (ë°°ì¹˜ ë˜ëŠ” ë¯¸ë‹ˆë°°ì¹˜)
ë“œë¡  ë¹„í–‰ê²½ë¡œ ê°ì‹œ	ë¹„í–‰ì œí•œêµ¬ì—­(ROI) ë‚´ ì§„ì… ì—¬ë¶€ í™•ì¸

âœ… ì°¸ê³  íŒ
ì…ë ¥ì´ ë§ë‹¤ë©´ Kafka íŒŒí‹°ì…˜ + Spark parallelism ì„¤ì • ê³ ë ¤

Sedonaì˜ ê³µê°„ ì¸ë±ì‹±ì€ RDDì—ì„œë§Œ ì‚¬ìš©ë¨ (SQL ê¸°ë°˜ì€ ì¸ë±ì‹± ìë™ ìµœì í™” ì—†ìŒ)

Spark Streamingì„ MicroBatch â†’ Continuous Processing ì „í™˜ ê°€ëŠ¥

